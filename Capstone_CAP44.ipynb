{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Project - CAP44\n",
    "# Team Members: CHING-YUAN PENG (cp4516), WEI-CHENG HSU (wh2757), and CHENG-JUI YANG (cy2941) .\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, classification_report, confusion_matrix, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "# Set random seed for reproducibility\n",
    "MY_SEED = 18358526\n",
    "\n",
    "random.seed(MY_SEED)\n",
    "np.random.seed(MY_SEED)\n",
    "rng = np.random.default_rng(MY_SEED)\n",
    "\n",
    "print(f\"Random Seed set to: {MY_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3e7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "num = pd.read_csv(\"rmpCapstoneNum.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.columns = ['Avg_Rating', 'Avg_Difficulty', 'NumofRatings', 'Pepper', 'Again?', 'From online', 'Male', 'Female']\n",
    "num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23601b46",
   "metadata": {},
   "source": [
    "Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num = ['AvgRating', 'AvgDifficulty', 'NumRatings', 'Pepper', 'ProportionRetake', 'OnlineRatings', 'Male', 'Female']\n",
    "cols_tags = [\n",
    "    \"Tough grader\", \"Good feedback\", \"Respected\", \"Lots to read\", \"Participation matters\",\n",
    "    \"Don't skip class\", \"Lots of homework\", \"Inspirational\", \"Pop quizzes!\", \"Accessible\",\n",
    "    \"So many papers\", \"Clear grading\", \"Hilarious\", \"Test heavy\", \"Graded by few things\",\n",
    "    \"Amazing lectures\", \"Caring\", \"Extra credit\", \"Group projects\", \"Lecture heavy\"\n",
    "]\n",
    "\n",
    "# 2. Load CSV files\n",
    "df_num = pd.read_csv('rmpCapstoneNum.csv', header=None, names=cols_num)\n",
    "df_tags = pd.read_csv('rmpCapstoneTags.csv', header=None, names=cols_tags)\n",
    "\n",
    "# 3. Merge datasets\n",
    "df = pd.concat([df_num, df_tags], axis=1)\n",
    "\n",
    "# 4. Data Cleaning\n",
    "# Remove rows with missing key information\n",
    "df = df.dropna(subset=['AvgRating', 'NumRatings', 'Male', 'Female'])\n",
    "# Ensure only Male or Female (exclude data errors)\n",
    "df = df[(df['Male'] == 1) | (df['Female'] == 1)]\n",
    "\n",
    "# [Key Filter] Only keep professors with >= 5 ratings (filter out extremely unstable samples)\n",
    "df_clean = df[df['NumRatings'] >= 5].copy()\n",
    "\n",
    "# Create a single Gender column for easier analysis and visualization\n",
    "df_clean['Gender'] = df_clean.apply(lambda x: 'Male' if x['Male'] == 1 else 'Female', axis=1)\n",
    "\n",
    "# Separate datasets\n",
    "male_ratings = df_clean[df_clean['Gender'] == 'Male']['AvgRating']\n",
    "female_ratings = df_clean[df_clean['Gender'] == 'Female']['AvgRating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fecc3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Perform statistical test: Mann-Whitney U test (non-parametric test)\n",
    "# The Mann-Whitney U test is used to compare the distributions of two independent groups\n",
    "# It tests the null hypothesis that the distributions are identical\n",
    "u_stat, p_val_q1 = stats.mannwhitneyu(male_ratings, female_ratings, alternative='two-sided')\n",
    "median_male = np.median(male_ratings)\n",
    "median_female = np.median(female_ratings)\n",
    "\n",
    "print(f\"--- Q1 Results ---\")\n",
    "print(f\"Male Median: {median_male}, Female Median: {median_female}\")\n",
    "print(f\"Mann-Whitney U Statistic: {u_stat}\")\n",
    "print(f\"P-value: {p_val_q1:.5e}\")\n",
    "\n",
    "# 2. Visualization: KDE Plot (Kernel Density Estimation plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df_clean, x='AvgRating', hue='Gender', fill=True, common_norm=False, palette=['blue', 'red'], alpha=0.3)\n",
    "plt.axvline(median_male, color='blue', linestyle='--', label=f'Male Median: {median_male}')\n",
    "plt.axvline(median_female, color='red', linestyle='--', label=f'Female Median: {median_female}')\n",
    "\n",
    "plt.title('Distribution of Average Ratings by Gender (Q1 Analysis)', fontsize=14)\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ffc51",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d533925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Perform KS Test (Kolmogorov-Smirnov test): Compare overall distributions/spread\n",
    "# The Kolmogorov-Smirnov test is a non-parametric test that compares the cumulative distribution functions (CDFs) of two samples\n",
    "# It tests the null hypothesis that the two samples are drawn from the same distribution\n",
    "ks_stat, p_val_q2 = stats.ks_2samp(male_ratings, female_ratings)\n",
    "std_male = np.std(male_ratings)\n",
    "std_female = np.std(female_ratings)\n",
    "\n",
    "print(f\"--- Q2 Results ---\")\n",
    "print(f\"KS Statistic: {ks_stat:.5f}\")\n",
    "print(f\"P-value: {p_val_q2:.5e}\")\n",
    "print(f\"Male Std Dev: {std_male:.4f}, Female Std Dev: {std_female:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.ecdfplot(data=df_clean, x='AvgRating', hue='Gender', palette=['blue', 'red'], linewidth=2)\n",
    "plt.title(f'Cumulative Distribution of Ratings (KS Test D={ks_stat:.3f})', fontsize=14)\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f069224",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap method: A resampling technique used to estimate the sampling distribution\n",
    "# of a statistic by repeatedly sampling with replacement from the observed data\n",
    "# This allows us to construct confidence intervals without assuming a specific distribution\n",
    "\n",
    "n_boot = 10000\n",
    "boot_median_diffs = []\n",
    "boot_std_diffs = []\n",
    "\n",
    "# Convert to numpy array for better performance\n",
    "m_data = male_ratings.values\n",
    "f_data = female_ratings.values\n",
    "\n",
    "for _ in range(n_boot):\n",
    "    # Resample with replacement\n",
    "    m_sample = np.random.choice(m_data, size=len(m_data), replace=True)\n",
    "    f_sample = np.random.choice(f_data, size=len(f_data), replace=True)\n",
    "    \n",
    "    # Calculate the difference in statistics for each simulation (Male - Female)\n",
    "    boot_median_diffs.append(np.median(m_sample) - np.median(f_sample)) # Corresponds to Q1\n",
    "    boot_std_diffs.append(np.std(m_sample) - np.std(f_sample))       # Corresponds to Q2\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "ci_median = np.percentile(boot_median_diffs, [2.5, 97.5])\n",
    "ci_std = np.percentile(boot_std_diffs, [2.5, 97.5])\n",
    "\n",
    "print(f\"--- Q3 Results ---\")\n",
    "print(f\"95% CI for Median Diff (Rating Bias): [{ci_median[0]:.4f}, {ci_median[1]:.4f}]\")\n",
    "print(f\"95% CI for Std Dev Diff (Spread Bias): [{ci_std[0]:.4f}, {ci_std[1]:.4f}]\")\n",
    "\n",
    "# Visualization: Bootstrapped Differences Histogram\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Median Difference\n",
    "sns.histplot(boot_median_diffs, kde=True, ax=ax[0], color='purple', alpha=0.5)\n",
    "ax[0].axvline(ci_median[0], color='red', linestyle='--')\n",
    "ax[0].axvline(ci_median[1], color='red', linestyle='--')\n",
    "ax[0].set_title('Bootstrap Dist. of Median Difference (Male - Female)')\n",
    "ax[0].set_xlabel('Difference in Median Rating')\n",
    "\n",
    "# Plot 2: Std Dev Difference\n",
    "sns.histplot(boot_std_diffs, kde=True, ax=ax[1], color='green', alpha=0.5)\n",
    "ax[1].axvline(ci_std[0], color='red', linestyle='--')\n",
    "ax[1].axvline(ci_std[1], color='red', linestyle='--')\n",
    "ax[1].set_title('Bootstrap Dist. of Std Dev Difference (Male - Female)')\n",
    "ax[1].set_xlabel('Difference in Standard Deviation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232e910",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Q4 Analysis Code: Chi-Square for Tags\n",
    "# ==========================================\n",
    "# Chi-Square test of independence: A statistical test used to determine if there is\n",
    "# a significant association between two categorical variables (Gender and Tag presence)\n",
    "# It tests the null hypothesis that the variables are independent\n",
    "\n",
    "# 1. Calculate total population (for normalization)\n",
    "total_ratings_male = df_clean[df_clean['Gender'] == 'Male']['NumRatings'].sum()\n",
    "total_ratings_female = df_clean[df_clean['Gender'] == 'Female']['NumRatings'].sum()\n",
    "\n",
    "# 2. Tag list\n",
    "cols_tags = [\n",
    "    \"Tough grader\", \"Good feedback\", \"Respected\", \"Lots to read\", \"Participation matters\",\n",
    "    \"Don't skip class\", \"Lots of homework\", \"Inspirational\", \"Pop quizzes!\", \"Accessible\",\n",
    "    \"So many papers\", \"Clear grading\", \"Hilarious\", \"Test heavy\", \"Graded by few things\",\n",
    "    \"Amazing lectures\", \"Caring\", \"Extra credit\", \"Group projects\", \"Lecture heavy\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# 3. Loop through each tag to perform the test\n",
    "for tag in cols_tags:\n",
    "    # Count how many times each gender received this tag\n",
    "    male_count = df_clean[df_clean['Gender'] == 'Male'][tag].sum()\n",
    "    female_count = df_clean[df_clean['Gender'] == 'Female'][tag].sum()\n",
    "    \n",
    "    # Create contingency table: [[Male with tag, Male without tag], [Female with tag, Female without tag]]\n",
    "    obs = np.array([\n",
    "        [male_count, total_ratings_male - male_count],\n",
    "        [female_count, total_ratings_female - female_count]\n",
    "    ])\n",
    "    \n",
    "    # Perform Chi-Square test of independence\n",
    "    chi2, p_val, dof, expected = stats.chi2_contingency(obs)\n",
    "    \n",
    "    # Calculate frequency difference (to determine if bias is toward Male or Female)\n",
    "    m_rate = male_count / total_ratings_male\n",
    "    f_rate = female_count / total_ratings_female\n",
    "    diff = m_rate - f_rate  # Positive value = Male bias, Negative value = Female bias\n",
    "    \n",
    "    results.append({\n",
    "        'Tag': tag,\n",
    "        'P-value': p_val,\n",
    "        'Diff': diff,\n",
    "        'Bias': 'Male' if diff > 0 else 'Female'\n",
    "    })\n",
    "\n",
    "# 4. Convert to DataFrame and sort by P-value\n",
    "df_res = pd.DataFrame(results).sort_values(by='P-value')\n",
    "\n",
    "print(\"\\nTop 3 Most Gendered (Lowest P-value):\")\n",
    "print(df_res.head(3)[['Tag', 'P-value', 'Bias']])\n",
    "\n",
    "print(\"\\nTop 3 Least Gendered (Highest P-value):\")\n",
    "print(df_res.tail(3)[['Tag', 'P-value', 'Bias']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bc69b",
   "metadata": {},
   "source": [
    "Question 5: Is there a gender difference in terms of average difficulty? Again, a significance test is indicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb22e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two gender columns into one\n",
    "num['Gender'] = num['Male'] + num['Female'] * -1\n",
    "num['Gender'] = num['Gender'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0 values with NaN in Gender column\n",
    "num['Gender'] = num['Gender'].replace(0, pd.NA)\n",
    "num['Gender'] = num['Gender'].replace(2, pd.NA)\n",
    "\n",
    "# Drop null values in Avg_Difficulty\n",
    "num = num.dropna(subset=['Avg_Difficulty', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Mann-Whitney U test to test if there is a significant difference in Avg_Difficulty between male and female professors\n",
    "\n",
    "# Use all data\n",
    "male_difficulty = num.loc[num['Gender'] == 1, 'Avg_Difficulty']\n",
    "female_difficulty = num.loc[num['Gender'] == -1, 'Avg_Difficulty']\n",
    "\n",
    "stat, p = mannwhitneyu(male_difficulty, female_difficulty, alternative='two-sided')\n",
    "print(\"Using all data:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "# Use data which has at least 2 ratings, since the last 25% of data has 2 rating\n",
    "male_difficulty = num.loc[(num['Gender'] == 1) & (num['NumofRatings'] >= 2), 'Avg_Difficulty']\n",
    "female_difficulty = num.loc[(num['Gender'] == -1) & (num['NumofRatings'] >= 2), 'Avg_Difficulty']\n",
    "\n",
    "stat, p = mannwhitneyu(male_difficulty, female_difficulty, alternative='two-sided')\n",
    "print(\"Using data with at least 2 ratings:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "# Use data which has at least 3 ratings, since the last 50% of data has less than 3 ratings\n",
    "male_difficulty = num.loc[(num['Gender'] == 1) & (num['NumofRatings'] >= 3), 'Avg_Difficulty']\n",
    "female_difficulty = num.loc[(num['Gender'] == -1) & (num['NumofRatings'] >= 3), 'Avg_Difficulty']\n",
    "\n",
    "stat, p = mannwhitneyu(male_difficulty, female_difficulty, alternative='two-sided')\n",
    "print(\"Using data with at least 3 ratings:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "# Use data which has at least 6 ratings, since the last 75% of data has less than 6 ratings\n",
    "male_difficulty = num.loc[(num['Gender'] == 1) & (num['NumofRatings'] >= 6), 'Avg_Difficulty']\n",
    "female_difficulty = num.loc[(num['Gender'] == -1) & (num['NumofRatings'] >= 6), 'Avg_Difficulty']\n",
    "\n",
    "stat, p = mannwhitneyu(male_difficulty, female_difficulty, alternative='two-sided')\n",
    "print(\"Using data with at least 6 ratings:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "# Use data which has at least 10 ratings\n",
    "male_difficulty = num.loc[(num['Gender'] == 1) & (num['NumofRatings'] >= 10), 'Avg_Difficulty']\n",
    "female_difficulty = num.loc[(num['Gender'] == -1) & (num['NumofRatings'] >= 10), 'Avg_Difficulty']\n",
    "\n",
    "stat, p = mannwhitneyu(male_difficulty, female_difficulty, alternative='two-sided')\n",
    "print(\"Using data with at least 10 ratings:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad793921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: KDE Plot (Kernel Density Estimation plot)\n",
    "male_median = np.median(num[num['Gender'] == 1]['Avg_Difficulty'])\n",
    "female_median = np.median(num[num['Gender'] == -1]['Avg_Difficulty'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=num, x='Avg_Difficulty', hue='Gender', fill=True, common_norm=False, palette=['blue', 'red'], alpha=0.3)\n",
    "plt.axvline(male_median, color='blue', linestyle='--', label=f'Male Median: {male_median}')\n",
    "plt.axvline(female_median, color='red', linestyle='--', label=f'Female Median: {female_median}')\n",
    "\n",
    "plt.title('Distribution of Average Difficulty by Gender (Q5 Analysis for all data)', fontsize=14)\n",
    "plt.xlabel('Average Difficulty')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410233e",
   "metadata": {},
   "source": [
    "Question 6: Please quantify the likely size of this effect at 95% confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_effect_size(num, threshold):\n",
    "    # Prepare data for Bootstrap\n",
    "    male_data = num.loc[(num['Gender'] == 1) & (num['NumofRatings'] >= threshold), 'Avg_Difficulty'].values\n",
    "    female_data = num.loc[(num['Gender'] == -1) & (num['NumofRatings'] >= threshold), 'Avg_Difficulty'].values\n",
    "\n",
    "    # Calculate the observed effect size\n",
    "    actual_diff = np.mean(male_data) - np.mean(female_data)\n",
    "\n",
    "    # Bootstrap process\n",
    "    n_boot = 10000\n",
    "    boot_diffs = []\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        # Resample with replacement\n",
    "        male_sample = rng.choice(male_data, size=len(male_data), replace=True)\n",
    "        female_sample = rng.choice(female_data, size=len(female_data), replace=True)\n",
    "\n",
    "        # Calculate the difference for this bootstrap sample\n",
    "        diff = np.mean(male_sample) - np.mean(female_sample)\n",
    "        boot_diffs.append(diff)\n",
    "\n",
    "    # Convert to array\n",
    "    boot_diffs = np.array(boot_diffs)\n",
    "\n",
    "    # Calculate 95% confidence interval\n",
    "    ci_lower = np.percentile(boot_diffs, 2.5)\n",
    "    ci_upper = np.percentile(boot_diffs, 97.5)\n",
    "\n",
    "    print(f\"Effect Size: {actual_diff:.4f}\")\n",
    "    print(f\"95% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in [1, 2, 3, 6, 10]:\n",
    "    print(f\"\\nBootstrap Effect Size for NumofRatings >= {threshold}:\")\n",
    "    bootstrap_effect_size(num, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82543382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Bootstrap\n",
    "male_data = num.loc[(num['Gender'] == 1) & (num['NumofRatings'] >= 1), 'Avg_Difficulty'].values\n",
    "female_data = num.loc[(num['Gender'] == -1) & (num['NumofRatings'] >= 1), 'Avg_Difficulty'].values\n",
    "\n",
    "# Calculate the observed effect size\n",
    "actual_diff = np.mean(male_data) - np.mean(female_data)\n",
    "\n",
    "# Bootstrap process\n",
    "n_boot = 10000\n",
    "boot_diffs = []\n",
    "\n",
    "for _ in range(n_boot):\n",
    "    # Resample with replacement\n",
    "    male_sample = rng.choice(male_data, size=len(male_data), replace=True)\n",
    "    female_sample = rng.choice(female_data, size=len(female_data), replace=True)\n",
    "\n",
    "    # Calculate the difference for this bootstrap sample\n",
    "    diff = np.mean(male_sample) - np.mean(female_sample)\n",
    "    boot_diffs.append(diff)\n",
    "\n",
    "# Convert to array\n",
    "boot_diffs = np.array(boot_diffs)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "ci_lower = np.percentile(boot_diffs, 2.5)\n",
    "ci_upper = np.percentile(boot_diffs, 97.5)\n",
    "\n",
    "print(f\"Effect Size: {actual_diff:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Visualization: Bootstrapped Differences Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(boot_diffs, kde=True, color='purple', alpha=0.5)\n",
    "plt.axvline(ci_lower, color='red', linestyle='--', label='95% CI Lower Bound')\n",
    "plt.axvline(ci_upper, color='red', linestyle='--', label='95% CI Upper Bound')\n",
    "plt.title(f'Bootstrap Distribution of Mean Difference (NumofRatings >= 1)')\n",
    "plt.xlabel('Difference in Mean Difficulty (Male - Female)') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35a6fb",
   "metadata": {},
   "source": [
    "Question 7: Build a regression model predicting average rating from all numerical predictors (the ones in the\n",
    "rmpCapstoneNum.csv) file. Make sure to include the R2 and RMSE of this model. Which of these\n",
    "factors is most strongly predictive of average rating? Hint: Make sure to address collinearity concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4217ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will check for multicollinearity among the features by calculating the correlation matrix, only correlation of the gender features are higher than 0.8\n",
    "# Mulicolinearity\n",
    "# --- Check for Multicollinearity ---\n",
    "# Select only the features (exclude target)\n",
    "features_for_collinearity = num.drop(columns=['Avg_Rating'])\n",
    "\n",
    "# Calculate correlation matrix among features, add labels\n",
    "feature_corr_matrix = features_for_collinearity.corr()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(feature_corr_matrix.columns)):\n",
    "    for j in range(i+1, len(feature_corr_matrix.columns)):\n",
    "        if abs(feature_corr_matrix.iloc[i, j]) > 0.8:  # threshold for high correlation\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': feature_corr_matrix.columns[i],\n",
    "                'Feature 2': feature_corr_matrix.columns[j],\n",
    "                'Correlation': feature_corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "print(\"\\nMulticollinearity Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Found {len(high_corr_pairs)} feature pairs with |r| > 0.8\")\n",
    "\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(\"\\nTop 10 most correlated feature pairs:\")\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs)\n",
    "    high_corr_df = high_corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "    print(high_corr_df.head(10))\n",
    "\n",
    "# Create heatmap for a subset of features\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(feature_corr_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Multicollinearity Heatmap', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "# Since feature male, female, and gender are collinear, we only keep gender\n",
    "predictors = ['Avg_Difficulty', 'NumofRatings', 'Pepper', 'Again?', 'From online', 'Gender']\n",
    "target = 'Avg_Rating'\n",
    "\n",
    "# Drop null values in predictors and target\n",
    "regression_data = num[predictors + [target]].dropna()\n",
    "\n",
    "print(f\"Original: {len(num)}\")\n",
    "print(f\"After removing null values: {len(regression_data)}\")\n",
    "\n",
    "# Define X and y\n",
    "X = regression_data[predictors]\n",
    "y = regression_data[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=MY_SEED)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Transform arrays back to DataFrames for statsmodels\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=predictors, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=predictors, index=X_test.index)\n",
    "\n",
    "# Add constant term for intercept\n",
    "X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y_train, X_train_scaled).fit()\n",
    "\n",
    "# Print the summary of the regression model\n",
    "print(model.summary())\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(\"Model R-squared on training set:\", model.rsquared)\n",
    "\n",
    "r2_score = r2_score(y_test, y_pred)\n",
    "print(\"Model R-squared on test set:\", r2_score)\n",
    "\n",
    "# Find the coefficients for Avg_Difficulty\n",
    "params = model.params.drop('const')\n",
    "max_impact_feature = params.abs().idxmax()\n",
    "max_impact_value = params[max_impact_feature]\n",
    "\n",
    "print(f\"The feature with the highest impact on Avg_Rating is '{max_impact_feature}' with a coefficient of {max_impact_value:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adad1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Plot 1: Predicted vs Actual Ratings (Model Accuracy) ---\n",
    "axes[0].scatter(y_test, y_pred, alpha=0.3, color='blue', label='Data Points')\n",
    "\n",
    "# Draw a red 45-degree line (perfect prediction line)\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "\n",
    "axes[0].set_title('Actual vs. Predicted Ratings', fontsize=15)\n",
    "axes[0].set_xlabel('Actual Avg_Rating', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Avg_Rating', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "\n",
    "# --- Plot 2: Strongest Predictor vs. Ratings (Visualizing Trends) ---\n",
    "strongest_factor = 'Again?'\n",
    "\n",
    "# To draw this plot, we use the original, unstandardized data (regression_data) for better readability\n",
    "plot_sample = regression_data.sample(n=min(2000, len(regression_data)), random_state=42)\n",
    "\n",
    "sns.regplot(\n",
    "    x=strongest_factor, \n",
    "    y='Avg_Rating', \n",
    "    data=plot_sample, \n",
    "    ax=axes[1], \n",
    "    scatter_kws={'alpha':0.3, 'color':'green'}, \n",
    "    line_kws={'color':'red'} \n",
    ")\n",
    "\n",
    "axes[1].set_title(f'Relationship: {strongest_factor} vs. Avg_Rating', fontsize=15)\n",
    "axes[1].set_xlabel(f'{strongest_factor} (Original Scale)', fontsize=12)\n",
    "axes[1].set_ylabel('Avg_Rating', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9db636",
   "metadata": {},
   "source": [
    "Bonus: Are the rating of professors in New York state different from those in other states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16224dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "num = pd.read_csv(\"rmpCapstoneNum.csv\", header=None)\n",
    "qual = pd.read_csv(\"rmpCapstoneQual.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c32a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns of qual for better readability\n",
    "num.columns = ['Avg_Rating', 'Avg_Difficulty', 'NumofRatings', 'Pepper', 'Again?', 'From online', 'Male', 'Female']\n",
    "qual.columns = ['Field', 'University', 'US State']\n",
    "qual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual[qual['US State'] == 'NY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24403310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge num and qual dataframes by index\n",
    "merged_data = num.merge(qual, left_index=True, right_index=True)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa974d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values in Avg_Rating, US State\n",
    "merged_data = merged_data.dropna(subset=['Avg_Rating', 'US State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take Mann-Whitney U test to test if there is a significant difference in Avg_Rating between different states\n",
    "\n",
    "# Use all data\n",
    "ny_ratings = merged_data.loc[merged_data['US State'] == 'NY', 'Avg_Rating']\n",
    "other_ratings = merged_data.loc[merged_data['US State'] != 'NY', 'Avg_Rating']\n",
    "\n",
    "stat, p = mannwhitneyu(ny_ratings, other_ratings, alternative='two-sided')\n",
    "print(\"Using all data:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "# Use data which has at least 2 ratings, since the last 25% of data has 1 rating\n",
    "ny_ratings = merged_data.loc[(merged_data['US State'] == 'NY') & (merged_data['NumofRatings'] >= 2), 'Avg_Rating']\n",
    "other_ratings = merged_data.loc[(merged_data['US State'] != 'NY') & (merged_data['NumofRatings'] >= 2), 'Avg_Rating']\n",
    "\n",
    "stat, p = mannwhitneyu(ny_ratings, other_ratings, alternative='two-sided')\n",
    "print(\"Using data with at least 2 ratings:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "# Use data which has at least 3 ratings, since the last 50% of data has less than 3 ratings\n",
    "ny_ratings = merged_data.loc[(merged_data['US State'] == 'NY') & (merged_data['NumofRatings'] >= 3), 'Avg_Rating']\n",
    "other_ratings = merged_data.loc[(merged_data['US State'] != 'NY') & (merged_data['NumofRatings'] >= 3), 'Avg_Rating']\n",
    "\n",
    "stat, p = mannwhitneyu(ny_ratings, other_ratings, alternative='two-sided')\n",
    "print(\"Using data with at least 3 ratings:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "# Use data which has at least 6 ratings, since the last 75% of data has less than 6 ratings\n",
    "ny_ratings = merged_data.loc[(merged_data['US State'] == 'NY') & (merged_data['NumofRatings'] >= 6), 'Avg_Rating']\n",
    "other_ratings = merged_data.loc[(merged_data['US State'] != 'NY') & (merged_data['NumofRatings'] >= 6), 'Avg_Rating']\n",
    "\n",
    "stat, p = mannwhitneyu(ny_ratings, other_ratings, alternative='two-sided')\n",
    "print(\"Using data with at least 6 ratings:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "# Use data which has at least 10 ratings\n",
    "ny_ratings = merged_data.loc[(merged_data['US State'] == 'NY') & (merged_data['NumofRatings'] >= 10), 'Avg_Rating']\n",
    "other_ratings = merged_data.loc[(merged_data['US State'] != 'NY') & (merged_data['NumofRatings'] >= 10), 'Avg_Rating']\n",
    "\n",
    "stat, p = mannwhitneyu(ny_ratings, other_ratings, alternative='two-sided')\n",
    "print(\"Using data with at least 10 ratings:\")\n",
    "print(f\"Mann-Whitney U test statistic: {stat}, p-value: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: KDE Plot (Kernel Density Estimation plot)\n",
    "ny_ratings = np.median(merged_data.loc[merged_data['US State'] == 'NY', 'Avg_Rating'])\n",
    "other_ratings = np.median(merged_data.loc[merged_data['US State'] != 'NY', 'Avg_Rating'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=merged_data, x='Avg_Rating', hue='US State', fill=True, common_norm=False, palette=['blue', 'red'], alpha=0.3)\n",
    "plt.axvline(ny_ratings, color='blue', linestyle='--', label=f'NY Median: {ny_ratings}')\n",
    "plt.axvline(other_ratings, color='red', linestyle='--', label=f'Other States Median: {other_ratings}')\n",
    "\n",
    "plt.title('Distribution of Average Rating by US State (Bonus Analysis for all data)', fontsize=14)\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc304c",
   "metadata": {},
   "source": [
    "Question8: Build a regression model predicting average ratings from all tags (the ones in the rmpCapstoneTags.csv) file. Make sure to include the R2 and RMSE of this model. Which of these tags is most strongly predictive of average rating? Hint: Make sure to address collinearity concerns. Also comment on how this model compares to the previous one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tags data\n",
    "tags = pd.read_csv(\"rmpCapstoneTags.csv\", header=None)\n",
    "\n",
    "# Define tag column names\n",
    "tag_names = [\n",
    "    'Tough_grader', 'Good_feedback', 'Respected', 'Lots_to_read', 'Participation_matters',\n",
    "    'Dont_skip_class', 'Lots_of_homework', 'Inspirational', 'Pop_quizzes', 'Accessible',\n",
    "    'So_many_papers', 'Clear_grading', 'Hilarious', 'Test_heavy', 'Graded_by_few_things',\n",
    "    'Amazing_lectures', 'Caring', 'Extra_credit', 'Group_projects', 'Lecture_heavy'\n",
    "]\n",
    "\n",
    "tags.columns = tag_names\n",
    "print(f\"Tags data shape: {tags.shape}\")\n",
    "tags.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload num data to ensure we have the full dataset\n",
    "num_full = pd.read_csv(\"rmpCapstoneNum.csv\", header=None)\n",
    "num_full.columns = ['Avg_Rating', 'Avg_Difficulty', 'NumofRatings', 'Pepper', 'Again?', 'From online', 'Male', 'Female']\n",
    "\n",
    "# Merge tags with numerical data by index\n",
    "tags_num = tags.merge(num_full[['Avg_Rating', 'NumofRatings']], left_index=True, right_index=True)\n",
    "\n",
    "# IMPORTANT: Handle average rating reliability based on number of ratings\n",
    "# Average ratings based on more ratings are more reliable\n",
    "# We will set a threshold: only use professors with at least 3 ratings\n",
    "# This is a judgment call - we choose 3 as a reasonable minimum\n",
    "MIN_RATINGS_THRESHOLD = 3\n",
    "print(f\"Setting minimum ratings threshold: {MIN_RATINGS_THRESHOLD}\")\n",
    "print(f\"Data before filtering: {len(tags_num)}\")\n",
    "tags_num = tags_num[tags_num['NumofRatings'] >= MIN_RATINGS_THRESHOLD]\n",
    "print(f\"Data after filtering (>= {MIN_RATINGS_THRESHOLD} ratings): {len(tags_num)}\")\n",
    "\n",
    "# Normalize tags by number of ratings\n",
    "# Since tags are raw counts and professors with more ratings will have more tags,\n",
    "# we normalize by dividing by the number of ratings (or use proportion)\n",
    "# We'll use proportion: tag_count / num_ratings\n",
    "# But we need to handle cases where num_ratings is 0 or NaN\n",
    "\n",
    "# Create normalized tag columns\n",
    "for tag in tag_names:\n",
    "    # Normalize: tag count per rating (proportion)\n",
    "    # If a student can award up to 3 tags, max tags per rating = 3\n",
    "    tags_num[f'{tag}_norm'] = tags_num[tag] / tags_num['NumofRatings'].replace(0, np.nan)\n",
    "\n",
    "print(\"\\nSample of normalized tags:\")\n",
    "tags_num[['Avg_Rating', 'NumofRatings'] + tag_names[:5] + [f'{tag}_norm' for tag in tag_names[:5]]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5338cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "# Use normalized tags as predictors\n",
    "tag_predictors = [f'{tag}_norm' for tag in tag_names]\n",
    "target = 'Avg_Rating'\n",
    "\n",
    "# Drop rows with missing values in target or any predictor\n",
    "# Keep NumofRatings for potential weighting\n",
    "regression_data_tags = tags_num[tag_predictors + [target, 'NumofRatings']].dropna()\n",
    "\n",
    "print(f\"Original data: {len(tags_num)}\")\n",
    "print(f\"After removing null values: {len(regression_data_tags)}\")\n",
    "print(f\"Note: Only using professors with >= {MIN_RATINGS_THRESHOLD} ratings for reliability\")\n",
    "\n",
    "# Check for multicollinearity among tags\n",
    "tag_corr_matrix = regression_data_tags[tag_predictors].corr()\n",
    "\n",
    "# Find highly correlated tag pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(tag_corr_matrix.columns)):\n",
    "    for j in range(i+1, len(tag_corr_matrix.columns)):\n",
    "        if abs(tag_corr_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'Tag 1': tag_corr_matrix.columns[i],\n",
    "                'Tag 2': tag_corr_matrix.columns[j],\n",
    "                'Correlation': tag_corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "print(f\"\\nFound {len(high_corr_pairs)} tag pairs with |r| > 0.8\")\n",
    "if len(high_corr_pairs) > 0:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs)\n",
    "    high_corr_df = high_corr_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "    print(high_corr_df.head(10))\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(tag_corr_matrix, cmap='coolwarm', center=0, square=True, \n",
    "            linewidths=0.5, cbar_kws={\"shrink\": 0.8}, fmt='.2f')\n",
    "plt.title('Tag Multicollinearity Heatmap', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build regression model\n",
    "X_tags = regression_data_tags[tag_predictors]\n",
    "y_tags = regression_data_tags[target]\n",
    "\n",
    "# Split data\n",
    "X_train_tags, X_test_tags, y_train_tags, y_test_tags = train_test_split(\n",
    "    X_tags, y_tags, test_size=0.2, random_state=MY_SEED\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_tags = StandardScaler()\n",
    "X_train_tags_scaled = scaler_tags.fit_transform(X_train_tags)\n",
    "X_test_tags_scaled = scaler_tags.transform(X_test_tags)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_tags_scaled = pd.DataFrame(X_train_tags_scaled, columns=tag_predictors, index=X_train_tags.index)\n",
    "X_test_tags_scaled = pd.DataFrame(X_test_tags_scaled, columns=tag_predictors, index=X_test_tags.index)\n",
    "\n",
    "# Add constant\n",
    "X_train_tags_scaled = sm.add_constant(X_train_tags_scaled)\n",
    "X_test_tags_scaled = sm.add_constant(X_test_tags_scaled)\n",
    "\n",
    "# Fit OLS model\n",
    "model_tags = sm.OLS(y_train_tags, X_train_tags_scaled).fit()\n",
    "\n",
    "# Print summary\n",
    "print(model_tags.summary())\n",
    "\n",
    "# Calculate RMSE and R² on test set\n",
    "from sklearn.metrics import r2_score\n",
    "y_pred_tags = model_tags.predict(X_test_tags_scaled)\n",
    "rmse_tags = np.sqrt(mean_squared_error(y_test_tags, y_pred_tags))\n",
    "r2_test_tags = r2_score(y_test_tags, y_pred_tags)\n",
    "print(f\"\\nTest RMSE: {rmse_tags:.4f}\")\n",
    "print(f\"Test R-squared: {r2_test_tags:.4f}\")\n",
    "print(f\"Training R-squared: {model_tags.rsquared:.4f}\")\n",
    "\n",
    "# Find most predictive tag and check significance (α = 0.005)\n",
    "ALPHA = 0.005\n",
    "params_tags = model_tags.params.drop('const')\n",
    "pvalues_tags = model_tags.pvalues.drop('const')\n",
    "\n",
    "# Find significant tags\n",
    "significant_tags = pvalues_tags[pvalues_tags < ALPHA]\n",
    "print(f\"\\nSignificant tags (p < {ALPHA}): {len(significant_tags)} out of {len(params_tags)}\")\n",
    "\n",
    "max_impact_tag = params_tags.abs().idxmax()\n",
    "max_impact_value = params_tags[max_impact_tag]\n",
    "max_impact_pvalue = pvalues_tags[max_impact_tag]\n",
    "\n",
    "print(f\"\\nThe tag with the highest impact on Avg_Rating is '{max_impact_tag}'\")\n",
    "print(f\"  Coefficient: {max_impact_value:.4f}\")\n",
    "print(f\"  P-value: {max_impact_pvalue:.10f}\")\n",
    "print(f\"  Significant at α={ALPHA}: {'Yes' if max_impact_pvalue < ALPHA else 'No'}\")\n",
    "\n",
    "# Compare with previous model (Question 7)\n",
    "print(f\"\\nComparison with Question 7 model:\")\n",
    "print(f\"Tags model R²: {model_tags.rsquared:.4f}\")\n",
    "print(f\"Numerical model R²: 0.809 (from Question 7)\")\n",
    "print(f\"Tags model RMSE: {rmse_tags:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for Question 8\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Predicted vs Actual\n",
    "axes[0].scatter(y_test_tags, y_pred_tags, alpha=0.3, color='purple', label='Data Points')\n",
    "min_val = min(y_test_tags.min(), y_pred_tags.min())\n",
    "max_val = max(y_test_tags.max(), y_pred_tags.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_title('Actual vs. Predicted Ratings (Tags Model)', fontsize=15)\n",
    "axes[0].set_xlabel('Actual Avg_Rating', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Avg_Rating', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Top 5 most predictive tags (by absolute coefficient)\n",
    "top_tags = params_tags.abs().nlargest(5)\n",
    "top_tags_sorted = top_tags.sort_values(ascending=True)\n",
    "axes[1].barh(range(len(top_tags_sorted)), top_tags_sorted.values, color='steelblue')\n",
    "axes[1].set_yticks(range(len(top_tags_sorted)))\n",
    "axes[1].set_yticklabels([tag.replace('_norm', '').replace('_', ' ') for tag in top_tags_sorted.index])\n",
    "axes[1].set_xlabel('Absolute Coefficient Value', fontsize=12)\n",
    "axes[1].set_title('Top 5 Most Predictive Tags', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc8311",
   "metadata": {},
   "source": [
    "Question 9: Build a regression model predicting average difficulty from all tags (the ones in the rmpCapstoneTags.csv) file. Make sure to include the R2 and RMSE of this model. Which of these tags is most strongly predictive of average difficulty? Hint: Make sure to address collinearity concerns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for difficulty prediction\n",
    "target_difficulty = 'Avg_Difficulty'\n",
    "\n",
    "# Merge tags with difficulty data\n",
    "tags_num_difficulty = tags.merge(num_full[['Avg_Difficulty', 'NumofRatings']], left_index=True, right_index=True)\n",
    "\n",
    "# Apply same threshold for rating reliability\n",
    "print(f\"Data before filtering: {len(tags_num_difficulty)}\")\n",
    "tags_num_difficulty = tags_num_difficulty[tags_num_difficulty['NumofRatings'] >= MIN_RATINGS_THRESHOLD]\n",
    "print(f\"Data after filtering (>= {MIN_RATINGS_THRESHOLD} ratings): {len(tags_num_difficulty)}\")\n",
    "\n",
    "# Create normalized tag columns (same normalization as before)\n",
    "for tag in tag_names:\n",
    "    tags_num_difficulty[f'{tag}_norm'] = tags_num_difficulty[tag] / tags_num_difficulty['NumofRatings'].replace(0, np.nan)\n",
    "\n",
    "# Prepare regression data\n",
    "regression_data_difficulty = tags_num_difficulty[tag_predictors + [target_difficulty, 'NumofRatings']].dropna()\n",
    "\n",
    "print(f\"Data for difficulty prediction: {len(regression_data_difficulty)} observations\")\n",
    "print(f\"Note: Only using professors with >= {MIN_RATINGS_THRESHOLD} ratings for reliability\")\n",
    "\n",
    "# Build regression model\n",
    "X_diff = regression_data_difficulty[tag_predictors]\n",
    "y_diff = regression_data_difficulty[target_difficulty]\n",
    "\n",
    "# Split data\n",
    "X_train_diff, X_test_diff, y_train_diff, y_test_diff = train_test_split(\n",
    "    X_diff, y_diff, test_size=0.2, random_state=MY_SEED\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_diff = StandardScaler()\n",
    "X_train_diff_scaled = scaler_diff.fit_transform(X_train_diff)\n",
    "X_test_diff_scaled = scaler_diff.transform(X_test_diff)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_diff_scaled = pd.DataFrame(X_train_diff_scaled, columns=tag_predictors, index=X_train_diff.index)\n",
    "X_test_diff_scaled = pd.DataFrame(X_test_diff_scaled, columns=tag_predictors, index=X_test_diff.index)\n",
    "\n",
    "# Add constant\n",
    "X_train_diff_scaled = sm.add_constant(X_train_diff_scaled)\n",
    "X_test_diff_scaled = sm.add_constant(X_test_diff_scaled)\n",
    "\n",
    "# Fit OLS model\n",
    "model_difficulty = sm.OLS(y_train_diff, X_train_diff_scaled).fit()\n",
    "\n",
    "# Print summary\n",
    "print(model_difficulty.summary())\n",
    "\n",
    "# Calculate RMSE and R² on test set\n",
    "y_pred_diff = model_difficulty.predict(X_test_diff_scaled)\n",
    "rmse_diff = np.sqrt(mean_squared_error(y_test_diff, y_pred_diff))\n",
    "r2_test_diff = r2_score(y_test_diff, y_pred_diff)\n",
    "print(f\"\\nTest RMSE: {rmse_diff:.4f}\")\n",
    "print(f\"Test R-squared: {r2_test_diff:.4f}\")\n",
    "print(f\"Training R-squared: {model_difficulty.rsquared:.4f}\")\n",
    "\n",
    "# Find most predictive tag and check significance (α = 0.005)\n",
    "ALPHA = 0.005\n",
    "params_diff = model_difficulty.params.drop('const')\n",
    "pvalues_diff = model_difficulty.pvalues.drop('const')\n",
    "\n",
    "# Find significant tags\n",
    "significant_tags_diff = pvalues_diff[pvalues_diff < ALPHA]\n",
    "print(f\"\\nSignificant tags (p < {ALPHA}): {len(significant_tags_diff)} out of {len(params_diff)}\")\n",
    "\n",
    "max_impact_tag_diff = params_diff.abs().idxmax()\n",
    "max_impact_value_diff = params_diff[max_impact_tag_diff]\n",
    "max_impact_pvalue_diff = pvalues_diff[max_impact_tag_diff]\n",
    "\n",
    "print(f\"\\nThe tag with the highest impact on Avg_Difficulty is '{max_impact_tag_diff}'\")\n",
    "print(f\"  Coefficient: {max_impact_value_diff:.4f}\")\n",
    "print(f\"  P-value: {max_impact_pvalue_diff:.6f}\")\n",
    "print(f\"  Significant at α={ALPHA}: {'Yes' if max_impact_pvalue_diff < ALPHA else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c93023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for Question 9\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Predicted vs Actual\n",
    "axes[0].scatter(y_test_diff, y_pred_diff, alpha=0.3, color='orange', label='Data Points')\n",
    "min_val = min(y_test_diff.min(), y_pred_diff.min())\n",
    "max_val = max(y_test_diff.max(), y_pred_diff.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_title('Actual vs. Predicted Difficulty (Tags Model)', fontsize=15)\n",
    "axes[0].set_xlabel('Actual Avg_Difficulty', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Avg_Difficulty', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Top 5 most predictive tags\n",
    "top_tags_diff = params_diff.abs().nlargest(5)\n",
    "top_tags_diff_sorted = top_tags_diff.sort_values(ascending=True)\n",
    "axes[1].barh(range(len(top_tags_diff_sorted)), top_tags_diff_sorted.values, color='coral')\n",
    "axes[1].set_yticks(range(len(top_tags_diff_sorted)))\n",
    "axes[1].set_yticklabels([tag.replace('_norm', '').replace('_', ' ') for tag in top_tags_diff_sorted.index])\n",
    "axes[1].set_xlabel('Absolute Coefficient Value', fontsize=12)\n",
    "axes[1].set_title('Top 5 Most Predictive Tags for Difficulty', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6549b",
   "metadata": {},
   "source": [
    "Question 10: Build a classification model that predicts whether a professor receives a \"pepper\" from all available factors (both tags and numerical). Make sure to include model quality metrics such as AU(RO)C and also address class imbalance concerns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "# Merge all data: tags (normalized), numerical features\n",
    "num_class = num_full.copy()\n",
    "num_class.columns = ['Avg_Rating', 'Avg_Difficulty', 'NumofRatings', 'Pepper', 'Again?', 'From online', 'Male', 'Female']\n",
    "\n",
    "# Apply threshold for rating reliability (same as regression models)\n",
    "print(f\"Data before filtering: {len(num_class)}\")\n",
    "num_class = num_class[num_class['NumofRatings'] >= MIN_RATINGS_THRESHOLD]\n",
    "print(f\"Data after filtering (>= {MIN_RATINGS_THRESHOLD} ratings): {len(num_class)}\")\n",
    "\n",
    "# Create Gender column\n",
    "num_class['Gender'] = num_class['Male'] + num_class['Female'] * -1\n",
    "num_class['Gender'] = num_class['Gender'].replace(0, pd.NA)\n",
    "num_class['Gender'] = num_class['Gender'].replace(2, pd.NA)\n",
    "\n",
    "# Merge tags with numerical data\n",
    "tags_class = tags.copy()\n",
    "tags_class.columns = tag_names\n",
    "\n",
    "# Normalize tags (divide by number of ratings)\n",
    "for tag in tag_names:\n",
    "    tags_class[f'{tag}_norm'] = tags_class[tag] / num_class['NumofRatings'].replace(0, np.nan)\n",
    "\n",
    "# Combine all features\n",
    "all_features = num_class[['Avg_Rating', 'Avg_Difficulty', 'NumofRatings', 'Again?', 'From online', 'Gender']].copy()\n",
    "all_features = all_features.merge(tags_class[[f'{tag}_norm' for tag in tag_names]], \n",
    "                                   left_index=True, right_index=True)\n",
    "\n",
    "# Target variable\n",
    "target_class = num_class['Pepper'].copy()\n",
    "\n",
    "# Combine and drop missing values\n",
    "classification_data = all_features.copy()\n",
    "classification_data['Pepper'] = target_class\n",
    "classification_data = classification_data.dropna()\n",
    "\n",
    "print(f\"\\nClassification data shape: {classification_data.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(classification_data['Pepper'].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(classification_data['Pepper'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3179d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_cols = [col for col in classification_data.columns if col != 'Pepper']\n",
    "X_class = classification_data[feature_cols]\n",
    "y_class = classification_data['Pepper'].astype(int)  # Ensure binary\n",
    "\n",
    "# Split data\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=MY_SEED, stratify=y_class\n",
    ")\n",
    "\n",
    "print(f\"Training set class distribution:\")\n",
    "print(y_train_class.value_counts())\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test_class.value_counts())\n",
    "\n",
    "# Standardize features\n",
    "scaler_class = StandardScaler()\n",
    "X_train_class_scaled = scaler_class.fit_transform(X_train_class)\n",
    "X_test_class_scaled = scaler_class.transform(X_test_class)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_class_scaled = pd.DataFrame(X_train_class_scaled, columns=feature_cols, index=X_train_class.index)\n",
    "X_test_class_scaled = pd.DataFrame(X_test_class_scaled, columns=feature_cols, index=X_test_class.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Logistic Regression model (with class_weight to handle imbalance)\n",
    "lr_model = LogisticRegression(random_state=MY_SEED, class_weight='balanced', max_iter=1000)\n",
    "lr_model.fit(X_train_class_scaled, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_class_scaled)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test_class_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "auc_lr = roc_auc_score(y_test_class, y_pred_proba_lr)\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"AUROC: {auc_lr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_class, y_pred_lr))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_class, y_pred_lr))\n",
    "\n",
    "# Feature importance (using absolute coefficients)\n",
    "lr_feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Abs_Coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for Question 10\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: ROC Curve for Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test_class, y_pred_proba_lr)\n",
    "\n",
    "axes[0].plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.3f})', linewidth=2, color='blue')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curve', fontsize=15)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Confusion Matrix for Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test_class, y_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[1], \n",
    "            xticklabels=['No Pepper', 'Pepper'], yticklabels=['No Pepper', 'Pepper'])\n",
    "axes[1].set_title('Confusion Matrix - Logistic Regression', fontsize=15)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Plot 3: Top 10 Feature Importances (Logistic Regression)\n",
    "top_10_features_lr = lr_feature_importance.head(10)\n",
    "axes[2].barh(range(len(top_10_features_lr)), top_10_features_lr['Abs_Coefficient'].values, color='steelblue')\n",
    "axes[2].set_yticks(range(len(top_10_features_lr)))\n",
    "axes[2].set_yticklabels([feat.replace('_norm', '').replace('_', ' ') for feat in top_10_features_lr['Feature']])\n",
    "axes[2].set_xlabel('Absolute Coefficient Value', fontsize=12)\n",
    "axes[2].set_title('Top 10 Most Important Features', fontsize=15)\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"Logistic Regression AUROC: {auc_lr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
